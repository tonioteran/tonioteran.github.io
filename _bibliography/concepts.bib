---
---
Concepts References
===================

@article{grisetti2010tutorial,
  title={A tutorial on graph-based SLAM},
  author={Grisetti, Giorgio and Kummerle, Rainer and Stachniss, Cyrill and Burgard, Wolfram},
  journal={IEEE Intelligent Transportation Systems Magazine},
  volume={2},
  number={4},
  pages={31--43},
  year={2010},
  publisher={IEEE}
}

@book{koller2009probabilistic,
  title={Probabilistic graphical models: principles and techniques},
  author={Koller, Daphne and Friedman, Nir},
  year={2009},
  publisher={MIT press}
}


@book{thrun2005probabilistic,
  title={Probabilistic robotics},
  author={Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  year={2005},
  publisher={MIT press}
}

@article{Holley2018,
author = {Holley, Peter},
journal = {The Washington Post},
month = {mar},
title = {{What autonomous vehicle companies are doing in the wake of Sunday's fatal Uber crash}},
url = {https://www.washingtonpost.com/news/innovations/wp/2018/03/23/what-autonomous-vehicle-companies-are-doing-in-the-wake-of-sundays-fatal-uber-crash/?noredirect=on{\&}utm{\_}term=.4ffd398b0d40},
year = {2018}
}


@article{Silva,
abstract = {Driverless vehicles operate by sensing and perceiving its surrounding environment to make the accurate driving decisions. A combination of several different sensors such as LiDAR, radar, ultrasound sensors and cameras are utilized to sense the surrounding environment of driverless vehicles. The heterogeneous sensors simultaneously capture various physical attributes of the environment. Such multimodality and redundancy of sensing need to be positively utilized for reliable and consistent perception of the environment through sensor data fusion. However, these multimodal sensor data streams are different from each other in many ways, such as temporal and spatial resolution, data format, and geometric alignment. For the subsequent perception algorithms to utilize the diversity offered by multimodal sensing, the data streams need to be spatially, geometrically and temporally aligned with each other. In this paper, we address the problem of fusing the outputs of a Light Detection and Ranging (LiDAR) scanner and a wide-angle monocular image sensor. The outputs of LiDAR scanner and the image sensor are of different spatial resolutions and need to be aligned with each other. A geometrical model is used to spatially align the two sensor outputs, followed by a Gaussian Process (GP) regression based resolution matching algorithm to interpolate the missing data with quantifiable uncertainty. The results indicate that the proposed sensor data fusion framework significantly aids the subsequent perception steps, as illustrated by the performance improvement of a typical free space detection algorithm.},
archivePrefix = {arXiv},
arxivId = {1710.06230},
author = {{De Silva}, Varuna and Roche, Jamie and Kondoz, Ahmet},
eprint = {1710.06230},
file = {:Users/annecollin 1/Library/Application Support/Mendeley Desktop/Downloaded/De Silva, Roche, Kondoz - 2017 - Fusion of LiDAR and Camera Sensor Data for Environment Sensing in Driverless Vehicles.pdf:pdf},
keywords = {Driverless cars,Free space detection,Gaussian Process Regression,Index Terms— Sensor data fusion,LiDAR,autonomous vehicles},
title = {{Fusion of LiDAR and Camera Sensor Data for Environment Sensing in Driverless Vehicles}},
url = {https://arxiv.org/pdf/1710.06230.pdf http://arxiv.org/abs/1710.06230},
year = {2017},
journal = {eprint arXiv:1710.06230}
}

@article{Quain2017,
author = {Quain, John},
journal = {The New York Times},
month = {may},
title = {{What Self-Driving Cars See}},
url = {https://www.nytimes.com/2017/05/25/automobiles/wheels/lidar-self-driving-cars.html},
year = {2017}
}

@unpublished{Canziani2016,
abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.},
archivePrefix = {arXiv},
arxivId = {1605.07678},
author = {Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio},
eprint = {1605.07678},
file = {:Users/annecollin 1/Library/Application Support/Mendeley Desktop/Downloaded/Canziani, Paszke, Culurciello - 2016 - An Analysis of Deep Neural Network Models for Practical Applications.pdf:pdf},
isbn = {2857825749},
month = {may},
pages = {1--7},
title = {{An Analysis of Deep Neural Network Models for Practical Applications}},
url = {http://arxiv.org/abs/1605.07678},
year = {2016}
}

@article{DeWeck2006,
abstract = {The design of technical systems such as automobiles and spacecraft has traditionally focused exclusively on performance maximization. Many organizations now realize that such an approach must be balanced against competing objectives of cost, risk, and other criteria. If one is willing to give up some amount of performance relative to the best achievable performance level, one introduces slack into system design. This slack can be invested in creating better outcomes overall. One way to achieve this is to balance the requirements among contributing subsystems such that the number of active constraints is minimized, while still achieving the desired system performance. This paper introduces a methodology called “isoperformance” as a means of identifying and evaluating a performance-invariant set of design solutions, which are efficient in terms of other criteria such as cost, risk, and lifecycle properties. Isoperformance is an inverse design method that starts from a desired vector of performance requirements and works backwards to identify acceptable solutions in the design space. To achieve this, gradient-based contour following is implemented as a multi- variable search algorithm that manipulates the null set of the Jacobian matrix. Use of the method is illustrated with two examples from spacecraft design and human performance in sports.},
author = {{De Weck}, Olivier L. and Jones, Marshall B.},
doi = {10.1002/sys.20043},
file = {:Users/annecollin 1/Library/Application Support/Mendeley Desktop/Downloaded/De Weck, Jones - 2006 - Isoperformance Analysis and design of complex systems with desired outcomes.pdf:pdf},
issn = {10981241},
journal = {Systems Engineering},
keywords = {Constraints,Gradient-based search,Inverse design methods,Isoperformance,Multiobjective optimization,Sensitivity analysis,System design},
number = {1},
pages = {45--61},
title = {{Isoperformance: Analysis and design of complex systems with desired outcomes}},
volume = {9},
year = {2006}
}

@inproceedings{Kschischang2013,
archivePrefix = {arXiv},
arxivId = {0018-9448/01},
author = {Kschischang, Frank R.},
booktitle = {IEEE Transactions on Information Theory},
doi = {0018-9448/01},
eprint = {01},
file = {:Users/annecollin 1/Library/Application Support/Mendeley Desktop/Downloaded/Kschischang - 2001 - Factor Graphs and the Sum-Product Algorithm.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {2},
pages = {498--519},
primaryClass = {0018-9448},
title = {{Factor Graphs and the Sum-Product Algorithm}},
volume = {47},
year = {2001}
}

@inproceedings{Meng2017,
author = {Meng, Huadong and Zhang, Wei-Bin},
booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
doi = {10.1109/IVS.2017.7995738},
file = {:Users/annecollin 1/Library/Application Support/Mendeley Desktop/Downloaded/Meng, Zhang - 2017 - Sensing architecture for automatedautonomous vehicles towards all-condition safety(2).pdf:pdf},
isbn = {978-1-5090-4804-5},
month = {jun},
number = {Iv},
pages = {317--321},
publisher = {IEEE},
title = {{Sensing architecture for automated/autonomous vehicles towards all-condition safety}},
url = {http://ieeexplore.ieee.org/document/7995738/},
year = {2017}
}

@unpublished{Tzoumas2017,
abstract = {Linear-Quadratic-Gaussian (LQG) control is concerned with the design of an optimal controller and estimator for linear Gaussian systems with imperfect state information. Standard LQG assumes the set of sensor measurements, to be fed to the estimator, to be given. However, in many problems, arising in networked systems and robotics, one may not be able to use all the available sensors, due to power or payload constraints, or may be interested in using the smallest subset of sensors that guarantees the attainment of a desired control goal. In this paper, we introduce the sensing-constrained LQG control problem, in which one has to jointly design sensing, estimation, and control, under given constraints on the resources spent for sensing. We focus on the realistic case in which the sensing strategy has to be selected among a finite set of possible sensing modalities. While the computation of the optimal sensing strategy is intractable, we present the first scalable algorithm that computes a near-optimal sensing strategy with provable sub-optimality guarantees. To this end, we show that a separation principle holds, which allows the design of sensing, estimation, and control policies in isolation. We conclude the paper by discussing two applications of sensing-constrained LQG control, namely, sensing-constrained formation control and resource-constrained robot navigation.},
archivePrefix = {arXiv},
arxivId = {1709.08826},
author = {Tzoumas, Vasileios and Carlone, Luca and Pappas, George J. and Jadbabaie, Ali},
doi = {https://doi.org/10.1109/CDC.2009.5400528},
eprint = {1709.08826},
file = {:Users/annecollin 1/Library/Application Support/Mendeley Desktop/Downloaded/Tzoumas et al. - 2017 - Sensing-Constrained LQG Control.pdf:pdf},
month = {sep},
pages = {14},
title = {{Sensing-Constrained LQG Control}},
url = {http://arxiv.org/abs/1709.08826},
year = {2017}
}

@inproceedings{Kehtarnavaz1991,
author = {Kehtarnavaz, Nasser and Griswold, Norman C. and Eem, J. K.},
booktitle = {Proceedings of SPIE},
doi = {10.1117/12.45489},
editor = {Trivedi, Mohan M.},
file = {:Users/annecollin/Documents/MIT/Research{\_}Neuro/References/Sensor{\_}comparison/Kehtarnavaz 1991 Comparison mono stereo.pdf:pdf},
month = {mar},
number = {March 1991},
pages = {467--478},
title = {{Comparison of mono- and stereo-camera systems for autonomous vehicle tracking}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=962882},
year = {1991}
}

@inproceedings{Gehrig2013,
abstract = {Autonomous Driving benefits strongly from a 3D reconstruction of the environment in real-time, often obtained via stereo vision. Semi-Global Matching (SGM) is a popular method of choice for solving this task which is already in use for production vehicles. Despite the enormous progress in the field and the high level of performance of modern methods, one key challenge remains: stereo vision in automotive scenarios during weather conditions such as rain, snow and night. Current methods generate strong temporal noise, many disparity outliers and false positives on a segmentation level. They are addressed in this work. We formulate a temporal prior and a scene prior, which we apply to SGM and Graph Cut. Using these priors, the object detection rate improves significantly on a driver assistance database of 3000 frames including bad weather while reducing the false positive rate. We also outperform the ECCV Robust Vision Challenge winner, iSGM, on this database.},
author = {Gehrig, Stefan and Reznitskii, Maxim and Schneider, Nicolai and Franke, Uwe and Weickert, Joachim},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCVW.2013.39},
file = {:Users/annecollin/Documents/MIT/Research{\_}Neuro/References/Sensor{\_}comparison/Gehrig 2013 Adverse conditions stereo.pdf:pdf},
isbn = {9781479930227},
keywords = {Computer Vision,Driver Assistance,Robustness,Stereo Vision},
pages = {238--245},
title = {{Priors for stereo vision under adverse weather conditions}},
year = {2013}
}

@article{Montgomery2006,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Ryde, Julian and Hillier, Nick},
doi = {10.1002/rob.20310},
eprint = {10.1.1.91.5767},
file = {:Users/annecollin/Documents/MIT/Research{\_}Neuro/References/Sensor{\_}comparison/Ryde 2009 Performance Radar Lidar.pdf:pdf},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {sep},
number = {9},
pages = {712--727},
pmid = {22164016},
title = {{Performance of laser and radar ranging devices in adverse environmental conditions}},
url = {http://doi.wiley.com/10.1002/rob.20310},
volume = {26},
year = {2009}
}

@inproceedings{RyanW2014,
abstract = {An apparatus and method for visual localization of a visual camera system outputting real-time visual camera data and a graphics processing unit receiving the real-time visual camera data. The graphics processing unit accesses a database of prior map information and generates a synthetic image that is then compared to the real-time visual camera data to determine corrected position data. The graphics processing unit determines a camera position based on the corrected position data. A corrective system for applying navigation of the vehicle based on the determined camera position can be used in some embodiments.},
author = {{Ryan W.}, Wolcott and {Ryan M.}, Eustice},
booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
file = {:Users/annecollin/Documents/MIT/Research{\_}Neuro/References/Sensor{\_}comparison/Wolcott 2014 Visual Localization Within LIDAR Maps.pdf:pdf},
isbn = {9781479969333},
keywords = {Field Robots,Localization,Visual Navigation},
number = {Iros},
pages = {8},
title = {{Visual Localization Within LIDAR Maps}},
year = {2014}
}

@inproceedings{Ma2012,
abstract = {We present a real-time system that enables a highly capable dynamic quadruped robot to maintain an accurate 6-DOF pose estimate (better than 0.5m over every 50m traveled) over long distances traversed through complex, dynamic outdoor terrain, during day and night, in the presence of camera occlusion and saturation, and occasional large external disturbances, such as slips or falls. The system fuses a stereo-camera sensor, inertial measurement units (IMU), and leg odometry with an Extended Kalman Filter (EKF) to ensure robust, low-latency performance. Extensive experimental results obtained from multiple field tests are presented to illustrate the performance and robustness of the system over hours of continuous runs over hundreds of meters of distance traveled in a wide variety of terrains and conditions.},
author = {Ma, Jeremy and Susca, Sara and Bajracharya, Max and Matthies, Larry and Malchano, Matt and Wooden, Dave},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6225132},
file = {:Users/annecollin/Documents/MIT/Research{\_}Neuro/References/Sensor{\_}comparison/Ma 2012 GPS-denied environments sensor fusion.pdf:pdf},
isbn = {978-1-4673-1405-3},
issn = {10504729},
month = {may},
pages = {619--626},
publisher = {IEEE},
title = {{Robust multi-sensor, day/night 6-DOF pose estimation for a dynamic legged vehicle in GPS-denied environments}},
url = {http://ieeexplore.ieee.org/document/6225132/},
year = {2012}
}

@article{Durrant-Whyte2006,
abstract = {This paper describes the simultaneous localization and mapping (SLAM) problem and the essential methods for solving the SLAM problem and summarizes key implementations and demonstrations of the method. While there are still many practical issues to overcome, especially in more complex outdoor environments, the general SLAM method is now a well understood and established part of robotics. Another part of the tutorial summarized more recent works in addressing some of the remaining issues in SLAM, including computation, feature representation, and data association},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Durrant-Whyte, Hugh and Bailey, Tim},
doi = {10.1109/MRA.2006.1638022},
eprint = {there is not},
file = {:Users/annecollin 1/Library/Application Support/Mendeley Desktop/Downloaded/Durrant-Whyte, Bailey - 2006 - Simultaneous localization and mapping part I.pdf:pdf},
isbn = {1070-9932},
issn = {1070-9932},
journal = {IEEE Robotics {\&} Automation Magazine},
number = {2},
pages = {99--110},
pmid = {1638022},
title = {{Simultaneous localization and mapping: part I}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1638022},
volume = {13},
year = {2006}
}

@article{ho2006loop,
  title={Loop closure detection in SLAM by combining visual and spatial appearance},
  author={Ho, Kin Leong and Newman, Paul},
  journal={Robotics and Autonomous Systems},
  volume={54},
  number={9},
  pages={740--749},
  year={2006},
  publisher={Elsevier}
}

@techreport{Dellaert2012,
abstract = {In this document I provide a hands-on introduction to both factor graphs and GTSAM. Factor graphs are graphical models (Koller and Friedman, 2009) that are well suited to mod- eling complex estimation problems, such as Simultaneous Localization and Mapping (SLAM) or Structure from Motion (SFM). You might be familiar with another often used graphical model, Bayes networks, which are directed acyclic graphs. A factor graph, however, is a bipartite graph consisting of factors connected to variables. The variables represent the unknown random vari- ables in the estimation problem, whereas the factors represent probabilistic information on those variables, derived from measurements or prior knowledge. In the following sections I will show many examples from both robotics and vision. The GTSAM toolbox (GTSAM stands for “Georgia Tech Smoothing and Mapping”) toolbox is a BSD-licensed C++ library based on factor graphs, developed at the Georgia Institute of Technol- ogy by myself, many of my students, and collaborators. It provides state of the art solutions to the SLAM and SFM problems, but can also be used to model and solve both simpler and more com- plex estimation problems. It also provides a MATLAB interface which allows for rapid prototype development, visualization, and user interaction. GTSAM exploits sparsity to be computationally efficient. Typically measurements only provide information on the relationship between a handful of variables, and hence the resulting factor graph will be sparsely connected. This is exploited by the algorithms implemented in GTSAM to reduce computational complexity. Even when graphs are too dense to be handled efficiently by direct methods, GTSAM provides iterative methods that are quite efficient regardless. You can download the latest version of GTSAM at http://tinyurl.com/gtsam.},
address = {Atlanta},
author = {Dellaert, Frank},
file = {:Users/annecollin 1/Library/Application Support/Mendeley Desktop/Downloaded/Dellaert - 2012 - Factor Graphs and GTSAM A Hands-on Introduction.pdf:pdf},
institution = {GeorgiaTech},
number = {September},
pages = {1--27},
title = {{Factor Graphs and GTSAM : A Hands-on Introduction}},
year = {2012}
}
